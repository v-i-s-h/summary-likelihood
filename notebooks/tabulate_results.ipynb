{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract eval results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze evaluation results for BMNIST\n",
    "- Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_results(model_dir):\n",
    "    \"\"\"\n",
    "        Get OOD metrics from model dir\n",
    "    \"\"\"\n",
    "\n",
    "    # Get config\n",
    "    config_json = os.path.join(model_dir, 'config.json')\n",
    "    config = json.load(open(config_json, 'r'))\n",
    "    \n",
    "    # Extract config values\n",
    "    method = config['method']\n",
    "    alpha = config['method_params'].get('alpha', 1.0)\n",
    "    ds_size = config['ds_params'].get('size', 'Full')\n",
    "    \n",
    "    \n",
    "    results = None\n",
    "    \n",
    "    # Get OOD result files\n",
    "    ood_result_files = glob.glob(model_dir + \"/ece_results_*.pkl\")\n",
    "    \n",
    "    # Get results\n",
    "    for rfile in ood_result_files:\n",
    "        filename = os.path.basename(rfile)\n",
    "        # Get corruption name from file name\n",
    "        corr_name = ' '.join(filename.split('_')[2:])[:-4]\n",
    "        with open(rfile, 'rb') as f:\n",
    "            logs = pickle.load(f)[0]\n",
    "            r = {\n",
    "                'method': method,\n",
    "                'alpha': alpha,\n",
    "                'ds_size': ds_size,\n",
    "                'corruption': corr_name,\n",
    "                'ece': logs['ece_uncal'],\n",
    "                'acc': logs['acc'],\n",
    "                'nll': logs['nll_uncal_test'],\n",
    "                'auroc': logs['auroc']\n",
    "            }\n",
    "            \n",
    "            if results is not None:\n",
    "                results.append(r)\n",
    "            else:\n",
    "                results = [r]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LeNet + 1000\n",
    "# models_root = \"./../zoo/sl/half-prior-alphavar/BinaryMNISTC-1000-53-identity/LeNet\"\n",
    "# models_root = \"./../zoo/sl/uniform-prior-alphavar/BinaryMNISTC-1000-53-identity/LeNet\"\n",
    "# models_root = \"./../zoo/sl/auto-prior-alphavar/BinaryMNISTC-1000-53-identity/LeNet\"\n",
    "# elbo_models_root = \"./../zoo/mfvi/BinaryMNISTC-1000-53-identity/LeNet\"\n",
    "# ls_models_root = \"./../zoo/ls/BinaryMNISTC-1000-53-identity/LeNet\"\n",
    "# edl_models_root = \"./../zoo/edl/BinaryMNISTC-1000-53-identity/LeNetEDL\"\n",
    "\n",
    "# # LeNet + 10000\n",
    "# models_root = \"./../zoo/sl/half-prior-alphavar/BinaryMNISTC-8000-53-identity/LeNet\"\n",
    "# models_root = \"./../zoo/sl/uniform-prior-alphavar/BinaryMNISTC-8000-53-identity/LeNet\"\n",
    "# models_root = \"./../zoo/sl/auto-prior-alphavar/BinaryMNISTC-8000-53-identity/LeNet\"\n",
    "# elbo_models_root = \"./../zoo/mfvi/BinaryMNISTC-8000-53-identity/LeNet\"\n",
    "# ls_models_root = \"./../zoo/ls/BinaryMNISTC-8000-53-identity/LeNet\"\n",
    "# edl_models_root = \"./../zoo/edl/BinaryMNISTC-8000-53-identity/LeNetEDL\"\n",
    "\n",
    "\n",
    "# ConvNet + 1000\n",
    "# models_root = \"./../zoo/sl/half-prior-alphavar/BinaryMNISTC-1000-53-identity/ConvNet\"\n",
    "# models_root = \"./../zoo/sl/auto-prior-alphavar/BinaryMNISTC-1000-53-identity/ConvNet\"\n",
    "# models_root = \"./../zoo/sl/uniform-prior-alphavar/BinaryMNISTC-1000-53-identity/ConvNet\"\n",
    "# elbo_models_root = \"./../zoo/mfvi/BinaryMNISTC-1000-53-identity/ConvNet\"\n",
    "# ls_models_root = \"./../zoo/ls/BinaryMNISTC-1000-53-identity/ConvNet\"\n",
    "# edl_models_root = \"./../zoo/edl/BinaryMNISTC-1000-53-identity/ConvNetEDL\"\n",
    "\n",
    "# ConvNet + 10000\n",
    "# models_root = \"./../zoo/sl/half-prior-alphavar/BinaryMNISTC-8000-53-identity/ConvNet\"\n",
    "# models_root = \"./../zoo/sl/auto-prior-alphavar/BinaryMNISTC-8000-53-identity/ConvNet\"\n",
    "models_root = \"./../zoo/sl/uniform-prior-alphavar/BinaryMNISTC-8000-53-identity/ConvNet\"\n",
    "elbo_models_root = \"./../zoo/mfvi/BinaryMNISTC-8000-53-identity/ConvNet\"\n",
    "ls_models_root = \"./../zoo/ls/BinaryMNISTC-8000-53-identity/ConvNet\"\n",
    "edl_models_root = \"./../zoo/edl/BinaryMNISTC-8000-53-identity/ConvNetEDL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-ELBO results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs = list(map(lambda d: os.path.join(models_root, d), os.listdir(models_root)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for _m in model_dirs:\n",
    "    results.extend(extract_results(_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBLO results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs = list(map(lambda d: os.path.join(elbo_models_root, d), os.listdir(elbo_models_root)))\n",
    "for _m in model_dirs:\n",
    "    results.extend(extract_results(_m))\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs = list(map(lambda d: os.path.join(ls_models_root, d), os.listdir(ls_models_root)))\n",
    "for _m in model_dirs:\n",
    "    results.extend(extract_results(_m))\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs = list(map(lambda d: os.path.join(edl_models_root, d), os.listdir(edl_models_root)))\n",
    "for _m in model_dirs:\n",
    "    results.extend(extract_results(_m))\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch up for additional methods\n",
    "df_results.alpha[df_results.method=='mfvi'] = -5.0 # For MFVI\n",
    "df_results.alpha[df_results.method=='ls'] = -1.0 # For label smoothing\n",
    "df_results.alpha[df_results.method=='edl'] = 0.0 # For EDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summ = df_results.groupby('alpha').agg(\n",
    "    n = pd.NamedAgg(column='acc', aggfunc='count'),\n",
    "    acc_mean = pd.NamedAgg(column='acc', aggfunc='mean'),\n",
    "    acc_err = pd.NamedAgg(column='acc', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    ece_mean = pd.NamedAgg(column='ece', aggfunc='mean'),\n",
    "    ece_err = pd.NamedAgg(column='ece', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    nll_mean = pd.NamedAgg(column='nll', aggfunc='mean'),\n",
    "    nll_err = pd.NamedAgg(column='nll', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    auroc_mean = pd.NamedAgg(column='auroc', aggfunc='mean'),\n",
    "    auroc_err = pd.NamedAgg(column='auroc', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_corr = df_results.groupby('corruption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfs = []\n",
    "for k, _df in gdf_corr:\n",
    "#     r1 = _df.groupby('lam_sl').agg({'corruption': 'first','ece': 'mean'}).sort_values(by='ece').reset_index()\n",
    "    r1 = _df.groupby('alpha').agg({\n",
    "                    'corruption': 'first', \n",
    "                    'acc': 'mean', \n",
    "                    'ece': 'mean', \n",
    "                    'nll': 'mean',\n",
    "                    'auroc': 'mean'}).reset_index()\n",
    "    r1['ece_rank'] = r1.ece.rank(ascending=True)\n",
    "    r1['acc_rank'] = r1.acc.rank(ascending=False)\n",
    "    r1['nll_rank'] = r1.nll.rank(ascending=True)\n",
    "    r1['auroc_rank'] = r1.auroc.rank(ascending=False)\n",
    "    rdfs.append(r1)\n",
    "\n",
    "df_ranked = pd.concat(rdfs)\n",
    "# df_ranked.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank_results = df_ranked.groupby('alpha').agg(\n",
    "    ece_rank_mean = pd.NamedAgg(column='ece_rank', aggfunc='mean'),\n",
    "    ece_rank_err = pd.NamedAgg(column='ece_rank', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    acc_rank_mean = pd.NamedAgg(column='acc_rank', aggfunc='mean'),\n",
    "    acc_rank_err = pd.NamedAgg(column='acc_rank', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    nll_rank_mean = pd.NamedAgg(column='nll_rank', aggfunc='mean'),\n",
    "    nll_rank_err = pd.NamedAgg(column='nll_rank', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    auroc_rank_mean = pd.NamedAgg(column='auroc_rank', aggfunc='mean'),\n",
    "    auroc_rank_err = pd.NamedAgg(column='auroc_rank', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = metrics_summ.merge(df_rank_results, on='alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printout final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_final.itertuples():\n",
    "    print(\n",
    "#         \"${:.0e}$\".format(row.Index),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.acc_mean, row.acc_err),\n",
    "        # \"& ${:.2f} \\pm {:.2f}$\".format(row.acc_rank_mean, row.acc_rank_err),\n",
    "        \"& ${:.2f}$\".format(row.acc_rank_mean),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.ece_mean, row.ece_err),\n",
    "#         \"& ${:.2f} \\pm {:.2f}$\".format(row.ece_rank_mean, row.ece_rank_err)\n",
    "        \"& ${:.2f}$\".format(row.ece_rank_mean)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create latex table for aggregate OOD performance over all corruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Table 13 - 16\n",
    "for row in df_final.itertuples():\n",
    "    print(\n",
    "        \"& ${:.3f} \\pm {:.3f} ({:.2f})$\".format(row.nll_mean, row.nll_err, row.nll_rank_mean),\n",
    "        \"& ${:.3f} \\pm {:.3f} ({:.2f})$\".format(row.acc_mean, row.acc_err, row.acc_rank_mean),\n",
    "        \"& ${:.3f} \\pm {:.3f} ({:.2f})$\".format(row.auroc_mean, row.auroc_err, row.auroc_rank_mean),\n",
    "        \"& ${:.3f} \\pm {:.3f} ({:.2f})$\".format(row.ece_mean, row.ece_err, row.ece_rank_mean)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For only identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iden = df_results[df_results.corruption == 'identity'].drop(['corruption'], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summ = df_iden.groupby('alpha').agg(\n",
    "    n = pd.NamedAgg(column='acc', aggfunc='count'),\n",
    "    acc_mean = pd.NamedAgg(column='acc', aggfunc='mean'),\n",
    "    acc_err = pd.NamedAgg(column='acc', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    ece_mean = pd.NamedAgg(column='ece', aggfunc='mean'),\n",
    "    ece_err = pd.NamedAgg(column='ece', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    nll_mean = pd.NamedAgg(column='nll', aggfunc='mean'),\n",
    "    nll_err = pd.NamedAgg(column='nll', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    auroc_mean = pd.NamedAgg(column='auroc', aggfunc='mean'),\n",
    "    auroc_err = pd.NamedAgg(column='auroc', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Table 1\n",
    "for row in metrics_summ.itertuples():\n",
    "    print(\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.nll_mean, row.nll_err),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.acc_mean, row.acc_err),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.auroc_mean, row.auroc_err),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.ece_mean, row.ece_err)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Table 7 - \n",
    "for row in metrics_summ.itertuples():\n",
    "    print(\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.nll_mean, row.nll_err),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.acc_mean, row.acc_err),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.auroc_mean, row.auroc_err),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.ece_mean, row.ece_err)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Bayes)",
   "language": "python",
   "name": "bayes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
