{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract eval results - CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy threshold for models to be included in analysis\n",
    "ACC_THRESHOLD = 0.75 # For CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_results(model_dir):\n",
    "    \"\"\"\n",
    "        Get metrics from model dir\n",
    "    \"\"\"\n",
    "\n",
    "    # Get config\n",
    "    config_json = os.path.join(model_dir, 'config.json')\n",
    "    config = json.load(open(config_json, 'r'))\n",
    "    \n",
    "    # Extract config values\n",
    "    method = config['method']\n",
    "    \n",
    "    # Create a table entry for parameters  as string\n",
    "    param_str = method # start with method label\n",
    "    for _p, _p_value in config['method_params'].items():\n",
    "        _p_value_str = None\n",
    "        if isinstance(_p_value, int):\n",
    "            _p_value_str = '{:04d}'.format(_p_value)\n",
    "        elif isinstance(_p_value, float):\n",
    "            _p_value_str = '{:08.3f}'.format(_p_value)\n",
    "        else:\n",
    "            _p_value_str = '{}'.format(_p_value)\n",
    "        param_str += '-{}={}'.format(_p, _p_value_str)\n",
    "    param = param_str\n",
    "    \n",
    "    results = None\n",
    "    \n",
    "    # Get result files\n",
    "    ood_result_files = glob.glob(model_dir + \"/ece_results_*.pkl\")\n",
    "    \n",
    "    # Get results\n",
    "    for rfile in ood_result_files:\n",
    "        filename = os.path.basename(rfile)\n",
    "        # Get corruption name from file name\n",
    "        corr_name = ' '.join(filename.split('_')[2:])[:-4]\n",
    "        # Split corruption name and identify severity\n",
    "        severity = 3 # default\n",
    "        _s = corr_name.split('-')\n",
    "        if len(_s) > 1:\n",
    "            corr_name = _s[0]\n",
    "            severity = _s[1]\n",
    "        with open(rfile, 'rb') as f:\n",
    "            logs = pickle.load(f)[0]\n",
    "            r = {\n",
    "                'method': method,\n",
    "                'params': param,\n",
    "                'corruption': corr_name,\n",
    "                'severity': severity,\n",
    "                'ece': logs['ece_uncal_val'],\n",
    "                'acc': logs['acc_val'],\n",
    "                'nll': logs['nll_uncal_val'],\n",
    "                'auroc': logs['auroc_val'],\n",
    "                'ece_test': logs['ece_uncal_test'],\n",
    "                'acc_test': logs['acc_test'],\n",
    "                'nll_test': logs['nll_uncal_test'],\n",
    "                'auroc_test': logs['auroc_test']\n",
    "            }\n",
    "            \n",
    "            if results is not None:\n",
    "                results.append(r)\n",
    "            else:\n",
    "                results = [r]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ood_results(model_dir, dataset_str='FMNIST'):\n",
    "    \"\"\"\n",
    "        Get OOD metrics from model dir\n",
    "    \"\"\"\n",
    "\n",
    "    # Get config\n",
    "    config_json = os.path.join(model_dir, 'config.json')\n",
    "    config = json.load(open(config_json, 'r'))\n",
    "    \n",
    "    # Extract config values\n",
    "    method = config['method']\n",
    "    \n",
    "    # Create a table entry for parameters  as string\n",
    "    param_str = method # start with method label\n",
    "    for _p, _p_value in config['method_params'].items():\n",
    "        _p_value_str = None\n",
    "        if isinstance(_p_value, int):\n",
    "            _p_value_str = '{:04d}'.format(_p_value)\n",
    "        elif isinstance(_p_value, float):\n",
    "            _p_value_str = '{:08.3f}'.format(_p_value)\n",
    "        else:\n",
    "            _p_value_str = '{}'.format(_p_value)\n",
    "        param_str += '-{}={}'.format(_p, _p_value_str)\n",
    "    param = param_str\n",
    "    \n",
    "    results = None\n",
    "    \n",
    "    # Get OOD result files\n",
    "    ood_result_files = glob.glob(model_dir + \"/ood_results_{}.pkl\".format(dataset_str))\n",
    "    \n",
    "    assert len(ood_result_files) <= 1, \"More than one OOD results exists\"\n",
    "\n",
    "    # Get accuracy on clean dataset also for quality checks\n",
    "    acc_results_file = os.path.join(model_dir, \"ece_results_identity-1.pkl\")\n",
    "    \n",
    "    # Get results\n",
    "    for rfile in ood_result_files:\n",
    "        with open(acc_results_file, 'rb') as f:\n",
    "            clean_results = pickle.load(f)[0]\n",
    "        with open(rfile, 'rb') as f:\n",
    "            logs = pickle.load(f)[0]\n",
    "            r = {\n",
    "                'method': method,\n",
    "                'params': param,\n",
    "                'ent_ood': logs['ent_ood'],\n",
    "                'ent_test': logs['ent_test'],\n",
    "                'ent_delta': logs['ent_delta'],\n",
    "                'acc': clean_results['acc_val']\n",
    "            }\n",
    "            \n",
    "            if results is not None:\n",
    "                results.append(r)\n",
    "            else:\n",
    "                results = [r]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 + VGG11\n",
    "result_dirs = [\n",
    "    \"./../zoo/multiclass-v2/sl/CIFAR10/VGG11\",\n",
    "    \"./../zoo/multiclass-v2/mfvi/CIFAR10/VGG11\",\n",
    "    # \"./../zoo/multiclass/ls/CIFAR10/VGG11\",\n",
    "    # \"./../zoo/multiclass/edl/computed-prior/CIFAR10/VGG11EDL\",\n",
    "    # \"./../zoo/multiclass/edl/skewed-prior/CIFAR10/VGG11EDL\",\n",
    "    \"./../zoo/multiclass-v2/edl/CIFAR10/VGG11EDL\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate model directories and load evaluation results\n",
    "results = []\n",
    "for models_root in result_dirs:\n",
    "    model_dirs = list(map(lambda d: os.path.join(models_root, d), os.listdir(models_root)))\n",
    "    for _m in model_dirs:\n",
    "        results.extend(extract_results(_m))\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do basic QA, ignore all models which fail to train satisfactorily\n",
    "df_results = df_results[df_results.acc > ACC_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results for Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_results[df_results.corruption == 'identity'].drop(['corruption'], axis=1).reset_index()\n",
    "# df_clean = df_results.drop(['corruption'], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summ = df_clean.groupby('params').agg(\n",
    "    n = pd.NamedAgg(column='acc', aggfunc='count'),\n",
    "    acc_mean = pd.NamedAgg(column='acc', aggfunc='mean'),\n",
    "    acc_err = pd.NamedAgg(column='acc', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    ece_mean = pd.NamedAgg(column='ece', aggfunc='mean'),\n",
    "    ece_err = pd.NamedAgg(column='ece', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    nll_mean = pd.NamedAgg(column='nll', aggfunc='mean'),\n",
    "    nll_err = pd.NamedAgg(column='nll', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    auroc_mean = pd.NamedAgg(column='auroc', aggfunc='mean'),\n",
    "    auroc_err = pd.NamedAgg(column='auroc', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Table 1\n",
    "\n",
    "nll_min = metrics_summ.nll_mean.min()\n",
    "acc_max = metrics_summ.acc_mean.max()\n",
    "auroc_max = metrics_summ.auroc_mean.max()\n",
    "ece_min = metrics_summ.ece_mean.min()\n",
    "\n",
    "for row in metrics_summ.itertuples():\n",
    "    buffer = \"{:50s}\".format(row.Index)\n",
    "\n",
    "    if row.nll_mean == nll_min:\n",
    "        buffer += \"& $\\mathbf{{{:.3f} \\pm {:.3f}}}$\".format(row.nll_mean, row.nll_err)\n",
    "    else:\n",
    "        buffer += \"&          ${:.3f} \\pm {:.3f}$\".format(row.nll_mean, row.nll_err)\n",
    "\n",
    "    if row.acc_mean == acc_max:\n",
    "        buffer += \"& $\\mathbf{{{:.3f} \\pm {:.3f}}}$\".format(row.acc_mean, row.acc_err)\n",
    "    else:\n",
    "        buffer += \"&          ${:.3f} \\pm {:.3f}$\".format(row.acc_mean, row.acc_err)\n",
    "\n",
    "    if row.auroc_mean == auroc_max:\n",
    "        buffer += \"& $\\mathbf{{{:.3f} \\pm {:.3f}}}$\".format(row.auroc_mean, row.auroc_err)\n",
    "    else:\n",
    "        buffer += \"&          ${:.3f} \\pm {:.3f}$\".format(row.auroc_mean, row.auroc_err)\n",
    "\n",
    "    if row.ece_mean == ece_min:\n",
    "        buffer += \"& $\\mathbf{{{:.3f} \\pm {:.3f}}}$\".format(row.ece_mean, row.ece_err)\n",
    "    else:\n",
    "        buffer += \"&          ${:.3f} \\pm {:.3f}$\".format(row.ece_mean, row.ece_err)\n",
    "\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get results for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summ_test = df_clean.groupby('params').agg(\n",
    "    n = pd.NamedAgg(column='acc_test', aggfunc='count'),\n",
    "    acc_mean = pd.NamedAgg(column='acc_test', aggfunc='mean'),\n",
    "    acc_err = pd.NamedAgg(column='acc_test', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    ece_mean = pd.NamedAgg(column='ece_test', aggfunc='mean'),\n",
    "    ece_err = pd.NamedAgg(column='ece_test', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    nll_mean = pd.NamedAgg(column='nll_test', aggfunc='mean'),\n",
    "    nll_err = pd.NamedAgg(column='nll_test', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    auroc_mean = pd.NamedAgg(column='auroc_test', aggfunc='mean'),\n",
    "    auroc_err = pd.NamedAgg(column='auroc_test', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summ_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefix(x):\n",
    "    parts = x.split('-')\n",
    "    if len(parts) < 2:\n",
    "        return '-'.join(parts)\n",
    "    else:\n",
    "        return '-'.join(parts[:-1])\n",
    "\n",
    "# Find groups of experiments without looking at last param\n",
    "unique_prefixes = list(sorted(set(\n",
    "        list(map(get_prefix, metrics_summ.index))\n",
    "    )))\n",
    "\n",
    "# For each of the unique prefixes, find the best in validation group according to\n",
    "# NLL\n",
    "r = []\n",
    "for pfx in unique_prefixes:\n",
    "    _df_val = metrics_summ[metrics_summ.index.str.startswith(pfx)]\n",
    "    idx = _df_val.nll_mean.idxmin()\n",
    "\n",
    "    # Now get the corresponding results from test set\n",
    "    r.append(metrics_summ_test.loc[idx])\n",
    "\n",
    "df_nll_best = pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_min = df_nll_best.nll_mean.min()\n",
    "acc_max = df_nll_best.acc_mean.max()\n",
    "auroc_max = df_nll_best.auroc_mean.max()\n",
    "ece_min = df_nll_best.ece_mean.min()\n",
    "\n",
    "for row in df_nll_best.itertuples():\n",
    "    buffer = \"{:50s}\".format(row.Index)\n",
    "\n",
    "    if row.nll_mean == nll_min:\n",
    "        buffer += \" & $\\mathbf{{{:.3f} \\pm {:.3f}}}$\".format(row.nll_mean, row.nll_err)\n",
    "    else:\n",
    "        buffer += \" &          ${:.3f} \\pm {:.3f}$\".format(row.nll_mean, row.nll_err)\n",
    "\n",
    "    if row.acc_mean == acc_max:\n",
    "        buffer += \" & $\\mathbf{{{:.3f} \\pm {:.3f}}}$\".format(row.acc_mean, row.acc_err)\n",
    "    else:\n",
    "        buffer += \" &          ${:.3f} \\pm {:.3f}$\".format(row.acc_mean, row.acc_err)\n",
    "\n",
    "    if row.auroc_mean == auroc_max:\n",
    "        buffer += \" & $\\mathbf{{{:.3f} \\pm {:.3f}}}$\".format(row.auroc_mean, row.auroc_err)\n",
    "    else:\n",
    "        buffer += \" &          ${:.3f} \\pm {:.3f}$\".format(row.auroc_mean, row.auroc_err)\n",
    "\n",
    "    if row.ece_mean == ece_min:\n",
    "        buffer += \" & $\\mathbf{{{:.3f} \\pm {:.3f}}}$\".format(row.ece_mean, row.ece_err)\n",
    "    else:\n",
    "        buffer += \" &          ${:.3f} \\pm {:.3f}$\".format(row.ece_mean, row.ece_err)\n",
    "\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate model directories and load evaluation results\n",
    "ood_results = []\n",
    "for models_root in result_dirs:\n",
    "    model_dirs = list(map(lambda d: os.path.join(models_root, d), os.listdir(models_root)))\n",
    "    for _m in model_dirs:\n",
    "        ood_results.extend(extract_ood_results(_m, dataset_str='SVHN'))\n",
    "df_ood_results = pd.DataFrame(ood_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ood_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ood_results = df_ood_results[df_ood_results.acc > ACC_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_metrics_summ = df_ood_results.groupby('params').agg(\n",
    "    n = pd.NamedAgg(column='ent_ood', aggfunc='count'),\n",
    "    ent_ood_mean = pd.NamedAgg(column='ent_ood', aggfunc='mean'),\n",
    "    ent_ood_err = pd.NamedAgg(column='ent_ood', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    ent_test_mean = pd.NamedAgg(column='ent_test', aggfunc='mean'),\n",
    "    ent_test_err = pd.NamedAgg(column='ent_test', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    ent_delta_mean = pd.NamedAgg(column='ent_delta', aggfunc='mean'),\n",
    "    ent_delta_err = pd.NamedAgg(column='ent_delta', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    ent_acc_mean = pd.NamedAgg(column='acc', aggfunc='mean'),\n",
    "    ent_acc_err = pd.NamedAgg(column='acc', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_metrics_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefix(x):\n",
    "    parts = x.split('-')\n",
    "    if len(parts) < 2:\n",
    "        return '-'.join(parts)\n",
    "    else:\n",
    "        return '-'.join(parts[:-1])\n",
    "\n",
    "# Find groups of experiments without looking at last param\n",
    "unique_prefixes = list(sorted(set(\n",
    "        list(map(get_prefix, ood_metrics_summ.index))\n",
    "    )))\n",
    "\n",
    "# For each of the unique prefixes, find the best in validation group according to\n",
    "# NLL and printout the entropy in OOD\n",
    "r = []\n",
    "for pfx in unique_prefixes:\n",
    "    _df_val = metrics_summ[metrics_summ.index.str.startswith(pfx)]\n",
    "    idx = _df_val.nll_mean.idxmin()\n",
    "\n",
    "    # Now get the corresponding results from OOD set\n",
    "    r.append(ood_metrics_summ.loc[idx])\n",
    "\n",
    "df_ood_best = pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ood_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table - Results OOD\n",
    "ent_delta_max = df_ood_best.ent_delta_mean.max()\n",
    "\n",
    "for row in df_ood_best.itertuples():\n",
    "    buffer = \"{:50s}\".format(row.Index)\n",
    "\n",
    "    # In-domain entropy\n",
    "    buffer += \"& ${:.3f} \\pm {:.3f}$\".format(row.ent_test_mean, row.ent_test_err)\n",
    "\n",
    "    # OOD entropy\n",
    "    buffer += \"& ${:.3f} \\pm {:.3f}$\".format(row.ent_ood_mean, row.ent_ood_err)\n",
    "\n",
    "    if row.ent_delta_mean == ent_delta_max:\n",
    "        buffer += \"& $\\mathbf{{{:.3f} \\pm {:.3f}}}$\".format(row.ent_delta_mean, row.ent_delta_err)\n",
    "    else:\n",
    "        buffer += \"&          ${:.3f} \\pm {:.3f}$\".format(row.ent_delta_mean, row.ent_delta_err)\n",
    "\n",
    "    print(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results for Corrupted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrupted = df_results[df_results.corruption != 'identity'].reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best method with NLL with each algorithm\n",
    "def get_prefix(x):\n",
    "    parts = x.split('-')\n",
    "    if len(parts) < 2:\n",
    "        return '-'.join(parts)\n",
    "    else:\n",
    "        return '-'.join(parts[:1])\n",
    "    # return parts[0]\n",
    "\n",
    "# Find groups of experiments without looking at last param\n",
    "unique_prefixes = list(sorted(set(\n",
    "        list(map(get_prefix, metrics_summ.index))\n",
    "    )))\n",
    "\n",
    "# For each of the unique prefixes, find the best in validation group according to\n",
    "# NLL\n",
    "r = []\n",
    "for pfx in unique_prefixes:\n",
    "    _df_val = metrics_summ[metrics_summ.index.str.startswith(pfx)]\n",
    "    idx = _df_val.nll_mean.idxmin()\n",
    "\n",
    "    # Now get the corresponding results from test set\n",
    "    r.append(metrics_summ_test.loc[idx])\n",
    "\n",
    "df_nll_best_config = pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nll_best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result for best configuration in family\n",
    "df_corrupted_test = df_corrupted[df_corrupted.params.isin(df_nll_best_config.index.tolist())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_corrupted_ece_mean = df_corrupted_test[\n",
    "    ['corruption', 'method', 'ece_test', 'nll_test', 'acc_test']\n",
    "].groupby(['corruption', 'method']).mean()\n",
    "\n",
    "df = gdf_corrupted_ece_mean.reset_index()\n",
    "df.method = df.method.str.upper()\n",
    "df = df.rename(columns={'ece_test': 'ECE', 'corruption': 'Corruption', 'method': 'Method'})\n",
    "\n",
    "df['Method'] = df['Method'].replace(['SL'], 'Proposed')\n",
    "df['Method'] = df['Method'].replace(['LS'], 'Label Smoothing')\n",
    "df['Method'] = df['Method'].replace(['MFVI'], 'ELBO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 3))\n",
    "g = sns.barplot(x='Corruption', y='ECE', hue='Method', \n",
    "            data=df)\n",
    "_ = g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "_ = g.set_xlabel(\"\")\n",
    "_ = g.set_ylabel(\"ECE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 3))\n",
    "g = sns.barplot(x='Corruption', y='nll_test', hue='Method', \n",
    "            data=df)\n",
    "_ = g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "_ = g.set_xlabel(\"\")\n",
    "_ = g.set_ylabel(\"NLL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data frame\n",
    "fig = plt.figure(figsize=(12, 3))\n",
    "g = sns.barplot(x='Corruption', y='acc_test', hue='Method', \n",
    "            data=df)\n",
    "_ = g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "_ = g.set_xlabel(\"\")\n",
    "# _ = g.set_ylim(0.5, 1.0)\n",
    "_ = g.set_ylabel(\"Accuracy\")\n",
    "_ = g.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level-wise corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptions = sorted(df_corrupted_test.corruption.unique())\n",
    "n = len(corruptions)\n",
    "\n",
    "df = df_corrupted_test[df_corrupted_test.corruption.isin(corruptions)]\n",
    "df['method'] = df['method'].replace(['sl'], 'Proposed')\n",
    "df['method'] = df['method'].replace(['ls'], 'Label Smoothing')\n",
    "df['method'] = df['method'].replace(['mfvi'], 'ELBO')\n",
    "df['method'] = df['method'].replace(['edl'], 'EDL')\n",
    "\n",
    "fig, ax = plt.subplots(n, 3, figsize=(3*4, n*2.5))\n",
    "\n",
    "gdf_corruption = df.groupby(by='corruption')\n",
    "\n",
    "for i, (_corr, _df) in enumerate(gdf_corruption):\n",
    "    gdf_method = _df.groupby(by='method')\n",
    "    for _method, _df_method in gdf_method:\n",
    "        _r = _df_method.groupby(by='severity').mean()\n",
    "\n",
    "        # Plot NLL\n",
    "        ax[i, 0].plot(_r.index, _r.nll_test, label=_method)\n",
    "\n",
    "        # Plot ECE\n",
    "        ax[i, 1].plot(_r.index, _r.ece_test, label=_method)\n",
    "\n",
    "        # Plot Acc\n",
    "        ax[i, 2].plot(_r.index, _r.acc_test, label=_method)\n",
    "\n",
    "    # Fix labels\n",
    "    if i == 0:\n",
    "        ax[i, 0].set_title(\"NLL\")\n",
    "        ax[i, 1].set_title(\"ECE\")\n",
    "        ax[i, 2].set_title(\"Acc\")\n",
    "    ax[i, 0].legend()\n",
    "    ax[i, 0].set_ylabel(_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Bayes)",
   "language": "python",
   "name": "bayes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
