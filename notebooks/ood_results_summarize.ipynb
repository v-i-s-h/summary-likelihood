{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract OOD test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_results(model_dir):\n",
    "    \"\"\"\n",
    "        Get OOD metrics from model dir\n",
    "    \"\"\"\n",
    "\n",
    "    # Get config\n",
    "    config_json = os.path.join(model_dir, 'config.json')\n",
    "    config = json.load(open(config_json, 'r'))\n",
    "    \n",
    "    # Extract config values\n",
    "    method = config['method']\n",
    "    lam_sl = config['method_params'].get('lam_sl', 0.0)\n",
    "    ds_size = config['ds_params'].get('size', 'Full')\n",
    "    \n",
    "    \n",
    "    results = None\n",
    "    \n",
    "    # Get OOD result files\n",
    "    ood_result_files = glob.glob(model_dir + \"/ece_results_*.pkl\")\n",
    "    \n",
    "    # Get results\n",
    "    for rfile in ood_result_files:\n",
    "        filename = os.path.basename(rfile)\n",
    "        # Get corruption name from file name\n",
    "        corr_name = ' '.join(filename.split('_')[2:])[:-4]\n",
    "        with open(rfile, 'rb') as f:\n",
    "            logs = pickle.load(f)[0]\n",
    "            r = {\n",
    "                'method': method,\n",
    "                'lam_sl': lam_sl,\n",
    "                'ds_size': ds_size,\n",
    "                'corruption': corr_name,\n",
    "                'ece': logs['ece_uncal'],\n",
    "                'acc': logs['acc']\n",
    "            }\n",
    "            \n",
    "            if results is not None:\n",
    "                results.append(r)\n",
    "            else:\n",
    "                results = [r]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet + 1000\n",
    "# models_root = \"./../zoo/abl-alpha100-uniform-lenet/BinaryMNISTC-1000-53-identity/LeNet\"\n",
    "# elbo_models_root = \"./../zoo/bmnist53-mfvi/BinaryMNISTC-1000-53-identity/LeNet\"\n",
    "elbo_models_root = \"./../zoo/bmnist53-ls/BinaryMNISTC-1000-53-identity/LeNet/\"\n",
    "\n",
    "# # LeNet + 10000\n",
    "# models_root = \"./../zoo/abl-alpha100-uniform-lenet/BinaryMNISTC-10000-53-identity/LeNet\"\n",
    "# elbo_models_root = \"./../zoo/bmnist53-mfvi/BinaryMNISTC-10000-53-identity/LeNet\"\n",
    "\n",
    "# # ConvNet + 1000\n",
    "# models_root = \"./../zoo/abl-alpha100-uniform-convnet/BinaryMNISTC-1000-53-identity/ConvNet\"\n",
    "# elbo_models_root = \"./../zoo/bmnist53-mfvi/BinaryMNISTC-1000-53-identity/ConvNet\"\n",
    "\n",
    "# # ConvNet + 10000\n",
    "# models_root = \"./../zoo/abl-alpha100-uniform-convnet/BinaryMNISTC-10000-53-identity/ConvNet\"\n",
    "# elbo_models_root = \"./../zoo/bmnist53-mfvi/BinaryMNISTC-10000-53-identity/ConvNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-ELBO results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_dirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m d: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(models_root, d), os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mmodels_root\u001b[49m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models_root' is not defined"
     ]
    }
   ],
   "source": [
    "model_dirs = list(map(lambda d: os.path.join(models_root, d), os.listdir(models_root)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dirs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_dirs\u001b[49m:\n\u001b[1;32m      3\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(extract_results(_m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_dirs' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for _m in model_dirs:\n",
    "    results.extend(extract_results(_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBLO results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs = list(map(lambda d: os.path.join(elbo_models_root, d), os.listdir(elbo_models_root)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../zoo/bmnist53-ls/BinaryMNISTC-1000-53-identity/LeNet/ls-sz1000-4-20220731135504\n",
      "./../zoo/bmnist53-ls/BinaryMNISTC-1000-53-identity/LeNet/ls-sz1000-5-20220731140041\n",
      "./../zoo/bmnist53-ls/BinaryMNISTC-1000-53-identity/LeNet/ls-sz1000-3-20220731134728\n",
      "./../zoo/bmnist53-ls/BinaryMNISTC-1000-53-identity/LeNet/ls-sz1000-2-20220731133423\n",
      "./../zoo/bmnist53-ls/BinaryMNISTC-1000-53-identity/LeNet/ls-sz1000-1-20220731132131\n"
     ]
    }
   ],
   "source": [
    "for _m in model_dirs:\n",
    "    print(_m)\n",
    "    results.extend(extract_results(_m))\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>lam_sl</th>\n",
       "      <th>ds_size</th>\n",
       "      <th>corruption</th>\n",
       "      <th>ece</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>impulse noise</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.987382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>stripe</td>\n",
       "      <td>0.059883</td>\n",
       "      <td>0.927445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>glass blur</td>\n",
       "      <td>0.021346</td>\n",
       "      <td>0.972135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>fog</td>\n",
       "      <td>0.047986</td>\n",
       "      <td>0.984753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>motion blur</td>\n",
       "      <td>0.015569</td>\n",
       "      <td>0.975815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>dotted line</td>\n",
       "      <td>0.010035</td>\n",
       "      <td>0.987907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>shear</td>\n",
       "      <td>0.027581</td>\n",
       "      <td>0.967928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>spatter</td>\n",
       "      <td>0.012282</td>\n",
       "      <td>0.986330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>brightness</td>\n",
       "      <td>0.009223</td>\n",
       "      <td>0.988959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>ls</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.993691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   method  lam_sl  ds_size     corruption       ece       acc\n",
       "0      ls     0.0     1000  impulse noise  0.010152  0.987382\n",
       "1      ls     0.0     1000         stripe  0.059883  0.927445\n",
       "2      ls     0.0     1000     glass blur  0.021346  0.972135\n",
       "3      ls     0.0     1000            fog  0.047986  0.984753\n",
       "4      ls     0.0     1000    motion blur  0.015569  0.975815\n",
       "..    ...     ...      ...            ...       ...       ...\n",
       "75     ls     0.0     1000    dotted line  0.010035  0.987907\n",
       "76     ls     0.0     1000          shear  0.027581  0.967928\n",
       "77     ls     0.0     1000        spatter  0.012282  0.986330\n",
       "78     ls     0.0     1000     brightness  0.009223  0.988959\n",
       "79     ls     0.0     1000       identity  0.007135  0.993691\n",
       "\n",
       "[80 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summ = df_results.groupby('lam_sl').agg(\n",
    "    n = pd.NamedAgg(column='acc', aggfunc='count'),\n",
    "    acc_mean = pd.NamedAgg(column='acc', aggfunc='mean'),\n",
    "    acc_err = pd.NamedAgg(column='acc', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    ece_mean = pd.NamedAgg(column='ece', aggfunc='mean'),\n",
    "    ece_err = pd.NamedAgg(column='ece', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_corr = df_results.groupby('corruption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdfs = []\n",
    "for k, _df in gdf_corr:\n",
    "#     r1 = _df.groupby('lam_sl').agg({'corruption': 'first','ece': 'mean'}).sort_values(by='ece').reset_index()\n",
    "    r1 = _df.groupby('lam_sl').agg({'corruption': 'first', 'acc': 'mean', 'ece': 'mean'}).reset_index()\n",
    "    r1['ece_rank'] = r1.ece.rank(ascending=True)\n",
    "    r1['acc_rank'] = r1.acc.rank(ascending=False)\n",
    "    rdfs.append(r1)\n",
    "\n",
    "df_ranked = pd.concat(rdfs)\n",
    "# df_ranked.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_err</th>\n",
       "      <th>ece_mean</th>\n",
       "      <th>ece_err</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lam_sl</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>80</td>\n",
       "      <td>0.955054</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>0.038909</td>\n",
       "      <td>0.006679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n  acc_mean   acc_err  ece_mean   ece_err\n",
       "lam_sl                                            \n",
       "0.0     80  0.955054  0.007182  0.038909  0.006679"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank_results = df_ranked.groupby('lam_sl').agg(\n",
    "    ece_rank_mean = pd.NamedAgg(column='ece_rank', aggfunc='mean'),\n",
    "    ece_rank_err = pd.NamedAgg(column='ece_rank', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0])),\n",
    "    acc_rank_mean = pd.NamedAgg(column='acc_rank', aggfunc='mean'),\n",
    "    acc_rank_err = pd.NamedAgg(column='acc_rank', aggfunc=lambda x: np.std(x) / np.sqrt(x.shape[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = metrics_summ.merge(df_rank_results, on='lam_sl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printout final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_err</th>\n",
       "      <th>ece_mean</th>\n",
       "      <th>ece_err</th>\n",
       "      <th>ece_rank_mean</th>\n",
       "      <th>ece_rank_err</th>\n",
       "      <th>acc_rank_mean</th>\n",
       "      <th>acc_rank_err</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lam_sl</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>80</td>\n",
       "      <td>0.955054</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>0.038909</td>\n",
       "      <td>0.006679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n  acc_mean   acc_err  ece_mean   ece_err  ece_rank_mean  \\\n",
       "lam_sl                                                              \n",
       "0.0     80  0.955054  0.007182  0.038909  0.006679            1.0   \n",
       "\n",
       "        ece_rank_err  acc_rank_mean  acc_rank_err  \n",
       "lam_sl                                             \n",
       "0.0              0.0            1.0           0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& $0.955 \\pm 0.007$ & $1.00$ & $0.039 \\pm 0.007$ & $1.00$\n"
     ]
    }
   ],
   "source": [
    "for row in df_final.itertuples():\n",
    "    print(\n",
    "#         \"${:.0e}$\".format(row.Index),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.acc_mean, row.acc_err),\n",
    "        # \"& ${:.2f} \\pm {:.2f}$\".format(row.acc_rank_mean, row.acc_rank_err),\n",
    "        \"& ${:.2f}$\".format(row.acc_rank_mean),\n",
    "        \"& ${:.3f} \\pm {:.3f}$\".format(row.ece_mean, row.ece_err),\n",
    "#         \"& ${:.2f} \\pm {:.2f}$\".format(row.ece_rank_mean, row.ece_rank_err)\n",
    "        \"& ${:.2f}$\".format(row.ece_rank_mean)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Bayes)",
   "language": "python",
   "name": "bayes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
