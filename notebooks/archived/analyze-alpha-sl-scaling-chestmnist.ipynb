{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of $\\alpha$ and $\\lambda_{SL}$ in Summary likelihood term\n",
    "\n",
    "This notebook explores the effect of concentration parameter $\\alpha$ summary likelihood term. The models are trained\n",
    "with `slurm-scripts/submit_mnistc_scale_alpha_sl.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "\n",
    "import methods\n",
    "import models\n",
    "import datasets\n",
    "import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_marginal_predictions(model, dataset, N=10):\n",
    "    dataloader = DataLoader(dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    # Get predictions from model\n",
    "    ytrue = []\n",
    "    ypreds = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            _ypreds, _ = model.sample_predictions(x, n=N)\n",
    "            if ypreds:\n",
    "                for i in range(N):\n",
    "                    ypreds[i] = torch.cat((ypreds[i], _ypreds[i]), dim=0)\n",
    "            else:\n",
    "                for y2 in _ypreds:\n",
    "                    # y2 is a tensor of shape (B, K)\n",
    "                    ypreds.append(y2)\n",
    "            ytrue.append(y)\n",
    "\n",
    "    # Convert to softmax score from log_softmax\n",
    "    yprobs = [torch.exp(_ypreds) for _ypreds in ypreds]\n",
    "    # Compute mean and std\n",
    "    yprob_marginal = torch.stack([_yprob.sum(dim=0) for _yprob in yprobs])\n",
    "    y_std, y_mean = torch.std_mean(yprob_marginal, dim=0)\n",
    "\n",
    "    # compure accuracy\n",
    "    ytrue = torch.cat(ytrue, dim=0)\n",
    "    acc = []\n",
    "    pred_entropy = []\n",
    "    ece = []\n",
    "    for _y in yprobs:\n",
    "        _acc = torchmetrics.functional.accuracy(_y, ytrue)\n",
    "        acc.append(_acc.numpy())\n",
    "        \n",
    "        _ent = torch.mean(torch.sum(_y * -torch.log(_y), dim=1))\n",
    "        pred_entropy.append(_ent.numpy())\n",
    "        \n",
    "        _ece = torchmetrics.functional.calibration_error(_y, ytrue, n_bins=10)\n",
    "        ece.append(_ece.numpy())\n",
    "    ll = []\n",
    "    for _y in ypreds: # ypreds are log-softmax\n",
    "        _ll = -torch.nn.functional.nll_loss(_y, ytrue)\n",
    "        ll.append(_ll.numpy())\n",
    "        \n",
    "    y_mean = y_mean.numpy()\n",
    "    y_std = y_std.numpy() / np.sqrt(N)\n",
    "\n",
    "    # accuracy\n",
    "    acc_mean = np.mean(acc)\n",
    "    acc_std = np.std(acc) / np.sqrt(N)\n",
    "    \n",
    "    # predictive entropy\n",
    "    ent_mean = np.mean(pred_entropy)\n",
    "    ent_std = np.std(pred_entropy) / np.sqrt(N)\n",
    "    \n",
    "    # ECE\n",
    "    ece_mean = np.mean(ece)\n",
    "    ece_std = np.std(ece) / np.sqrt(N)\n",
    "    \n",
    "    # LL\n",
    "    ll_mean = np.mean(ll)\n",
    "    ll_std = np.std(ll) / np.sqrt(N)\n",
    "    \n",
    "    return (\n",
    "        y_mean, y_std, \n",
    "        acc_mean, acc_std, \n",
    "        ent_mean, ent_std, \n",
    "        ece_mean, ece_std,\n",
    "        ll_mean, ll_std\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./../zoo/alpha-sl-scaling/ChestMNIST-infiltration/LeNet/auto-ea075\"\n",
    "\n",
    "# model_dirs = [f for f in os.listdir(root_dir) if re.match(r'.+-nw-[1]-\\d+', f)]\n",
    "model_dirs = os.listdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = namedtuple(\"Result\",\n",
    "    \"model_id lambda_sl alpha corruption \" \n",
    "    \"y_mean_0 y_mean_1 y_std_0 y_std_1 \" \n",
    "    \"acc_mean acc_std \" \n",
    "    \"ent_mean ent_std \"\n",
    "    \"ece_mean ece_std \"\n",
    "    \"ll_mean ll_std\")\n",
    "results = []\n",
    "for _model_dir in model_dirs:\n",
    "    model_dir = os.path.join(root_dir, _model_dir)\n",
    "    # Default paths\n",
    "    config_json = os.path.join(model_dir, \"config.json\")\n",
    "    ckpt_file = os.path.join(model_dir, \"last.ckpt\")\n",
    "\n",
    "    config = json.load(open(config_json, 'r'))\n",
    "\n",
    "    MethodClass = getattr(methods, config['method'])\n",
    "    DatasetClass = getattr(datasets, config['dataset'])\n",
    "    ModelClass = getattr(models, config['model'])\n",
    "    TransformClass = getattr(transforms, config['transform'])\n",
    "\n",
    "    testset = DatasetClass(**config['ds_params'], split='test', transform=TransformClass())\n",
    "    K = testset.n_labels\n",
    "\n",
    "    model = MethodClass.load_from_checkpoint(\n",
    "            os.path.join(model_dir, \"last.ckpt\"),\n",
    "            model=ModelClass(K))\n",
    "    \n",
    "    # Test for different corruptions\n",
    "#     for corruption in testset.corruptions:\n",
    "    for corruption in ['identity']:\n",
    "        testset = DatasetClass(\n",
    "                    config['ds_params']['label'], \n",
    "                    split='test', \n",
    "#                     corruption=corruption,\n",
    "                    transform=TransformClass())\n",
    "\n",
    "        (\n",
    "            y_mean, y_std, \n",
    "            acc_mean, acc_std, \n",
    "            ent_mean, ent_std, \n",
    "            ece_mean, ece_std,\n",
    "            ll_mean, ll_std\n",
    "        ) = get_marginal_predictions(model, testset, N=50)\n",
    "\n",
    "    \n",
    "        results.append(\n",
    "            Result(\n",
    "                _model_dir, model.lam_sl, model.alpha, corruption,\n",
    "                y_mean[0], y_mean[1], y_std[0], y_std[1],\n",
    "                acc_mean, acc_std,\n",
    "                ent_mean, ent_std,\n",
    "                ece_mean, ece_std,\n",
    "                ll_mean, ll_std\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results).sort_values(by='alpha').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_pickle(\"analyze-alpha-sl-scaling.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.read_pickle(\"./analyze-alpha-sl-scaling.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_val = sorted(df_results['alpha'].unique().tolist())\n",
    "lamsl_val = sorted(df_results['lambda_sl'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick colors for different \\alpha values\n",
    "pen_colors = plt.cm.get_cmap('Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictive marginal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_results[df_results.corruption == 'identity']\n",
    "gdf = df.groupby(by='lambda_sl')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "legend_dict = {}\n",
    "for i, (k, _df) in enumerate(gdf):\n",
    "    gdf2 = _df.groupby(by='alpha')\n",
    "    for k2, _df2 in gdf2:\n",
    "        y_mean = _df2.y_mean_1\n",
    "        x = [k2] * len(y_mean)\n",
    "        ax = plt.scatter(x, y_mean, marker='o', s= 10 + 10 * _df2.y_std_1, \n",
    "                            color=pen_colors(i), label=\"$\\\\lambda = {:1.0e}$\".format(k))\n",
    "        if k not in legend_dict:\n",
    "            legend_dict[k] = ax\n",
    "\n",
    "plt.hlines(testset.n_classes[1], \n",
    "            1e-3, 1e+2,\n",
    "            colors='k', linestyles=':')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Predictive marginal distribution\")\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"Predictive probability\")\n",
    "plt.legend(handles=legend_dict.values())\n",
    "plt.savefig(\"pred-mar-vs-alpha-sl.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictive accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_results[df_results.corruption == 'identity']\n",
    "# gdf = df.groupby(by='lambda_sl')\n",
    "gdf = df.groupby(by='alpha')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "legend_dict = {}\n",
    "for i, (k, _df) in enumerate(gdf):\n",
    "#     gdf2 = _df.groupby(by='alpha')\n",
    "    gdf2 = _df.groupby(by='lambda_sl')\n",
    "    for k2, _df2 in gdf2:\n",
    "        y_mean = _df2.acc_mean\n",
    "        x = [k2] * len(y_mean)\n",
    "        ax = plt.scatter(x, y_mean, marker='o', s= 50 + 1e4 * _df2.acc_std, \n",
    "                            color=pen_colors(i), label=\"$\\\\lambda = {:1.0e}$\".format(k))\n",
    "        if k not in legend_dict:\n",
    "            legend_dict[k] = ax\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title(\"Accuracy of model\")\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(handles=legend_dict.values())\n",
    "plt.savefig(\"pred-acc-vs-alpha-sl.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictive entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_results[df_results.corruption == 'identity']\n",
    "gdf = df.groupby(by='lambda_sl')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "legend_dict = {}\n",
    "for i, (k, _df) in enumerate(gdf):\n",
    "    gdf2 = _df.groupby(by='alpha')\n",
    "    for k2, _df2 in gdf2:\n",
    "        y_mean = _df2.ent_mean\n",
    "        x = [k2] * len(y_mean)\n",
    "        ax = plt.scatter(x, y_mean, marker='o', s= 50 + 1e4 * _df2.ent_std, \n",
    "                            color=pen_colors(i), label=\"$\\\\lambda = {:1.0e}$\".format(k))\n",
    "        if k not in legend_dict:\n",
    "            legend_dict[k] = ax\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title(\"Predictive entropy\")\n",
    "plt.xlabel(\"$\\\\alpha$\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.legend(handles=legend_dict.values())\n",
    "plt.savefig(\"pred-ent-vs-alpha-sl.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot preditive loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_results[df_results.corruption == 'identity']\n",
    "# gdf = df.groupby(by='lambda_sl')\n",
    "gdf = df.groupby(by='alpha')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "legend_dict = {}\n",
    "for i, (k, _df) in enumerate(gdf):\n",
    "    # gdf2 = _df.groupby(by='alpha')\n",
    "    gdf2 = _df.groupby(by='lambda_sl')\n",
    "    for k2, _df2 in gdf2:\n",
    "        y_mean = _df2.ll_mean\n",
    "        x = [k2] * len(y_mean)\n",
    "        ax = plt.scatter(x, y_mean, marker='o', s= 30 + 1e3 * _df2.ll_std, \n",
    "                            color=pen_colors(i), label=\"$\\\\alpha = {:1.0e}$\".format(k))\n",
    "        if k not in legend_dict:\n",
    "            legend_dict[k] = ax\n",
    "plt.xscale('log')\n",
    "plt.title(\"Loglikelihood\")\n",
    "plt.xlabel(\"$\\\\lambda_{SL}$\")\n",
    "plt.ylabel(\"Loglikelihood\")\n",
    "plt.legend(handles=legend_dict.values())\n",
    "plt.savefig(\"ll-vs-alpha-sl.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot expected calibration error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_results[df_results.corruption == 'identity']\n",
    "# gdf = df.groupby(by='lambda_sl')\n",
    "gdf = df.groupby(by='alpha')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "legend_dict = {}\n",
    "for i, (k, _df) in enumerate(gdf):\n",
    "#     gdf2 = _df.groupby(by='alpha')\n",
    "    gdf2 = _df.groupby(by='lambda_sl')\n",
    "    for k2, _df2 in gdf2:\n",
    "        y_mean = _df2.ece_mean\n",
    "        x = [k2] * len(y_mean)\n",
    "        ax = plt.scatter(x, y_mean, marker='o', s= 30 + 1e4 * _df2.ece_std, \n",
    "                            color=pen_colors(i),\n",
    "                            label=\"$\\\\alpha = {:1.0e}$\".format(k))\n",
    "        if k not in legend_dict:\n",
    "            legend_dict[k] = ax\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title(\"ECE\")\n",
    "plt.xlabel(\"$\\\\lambda_{SL}$\")\n",
    "plt.ylabel(\"Expected Calibration Error\")\n",
    "plt.legend(handles=legend_dict.values())\n",
    "plt.savefig(\"ece-vs-alpha-sl.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cc81e65ac001079a8dbd6b9a2e1804ae2bdf52e129f39c92608ef96e247326a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bayes')",
   "language": "python",
   "name": "python397jvsc74a57bd013ede2aa3846a80420e8be59016e3b86d734ba47be24b4292330ac6ad96136da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
