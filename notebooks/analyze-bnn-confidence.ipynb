{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze BNN confidence on corrupted data\n",
    "\n",
    "This experiment analyzes the confidence of BNN predictions on a corrupted data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import models\n",
    "from transforms import normalize_x\n",
    "import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_blur(std=1.0):\n",
    "    \"\"\"\n",
    "        Normalize and blur an image\n",
    "    \"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.0], std=[1.0]),\n",
    "        transforms.GaussianBlur(13, sigma=std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transform = normalize_x()\n",
    "trainset = datasets.BinaryMNISTC(53, 'identity', 'train', transform=normalize_x())\n",
    "testset = datasets.BinaryMNISTC(53, 'identity', 'test', transform=normalize_x())\n",
    "\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LeNet(K=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        output_ = []\n",
    "        \n",
    "        for mc_run in range(16):\n",
    "            output, kl = model(data)\n",
    "            output_.append(output)\n",
    "        \n",
    "        output = torch.mean(torch.stack(output_), dim=0)\n",
    "        nll_loss = F.nll_loss(output, target)\n",
    "        \n",
    "        #ELBO loss\n",
    "        loss = nll_loss +  1 / kl / 64\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testloader):\n",
    "    pred_probs_mc = []\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    pred_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            mc_samples = []\n",
    "            for mc_run in range(16):\n",
    "                model.eval()\n",
    "                output, _ = model.forward(data)\n",
    "                #get probabilities from log-prob\n",
    "                pred_probs = torch.exp(output)\n",
    "                mc_samples.append(pred_probs.cpu().data.numpy())\n",
    "\n",
    "            target_labels = target.cpu().data.numpy()\n",
    "            pred_mean = np.mean(mc_samples, axis=0)\n",
    "            Y_pred = np.argmax(pred_mean, axis=1)\n",
    "\n",
    "            pred_probs_mc.append(np.stack(mc_samples, axis=1))\n",
    "            targets.append(target_labels)\n",
    "            predictions.append(pred_mean)\n",
    "            pred_labels.append(Y_pred)\n",
    "\n",
    "    # Stack all results\n",
    "    pred_probs_mc = np.vstack(pred_probs_mc)\n",
    "    targets = np.hstack(targets)\n",
    "    predictions = np.vstack(predictions)\n",
    "    pred_labels = np.hstack(pred_labels)\n",
    "    \n",
    "    return pred_labels, targets, predictions, pred_probs_mc\n",
    "\n",
    "pred_labels, targets, predictions, pred_probs_mc = evaluate_model(model, testloader)\n",
    "print(\"Test accuracy: \", (targets == pred_labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop p's for label 0\n",
    "pred_probs_mc = pred_probs_mc[:, :, 1]\n",
    "predictions = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of predictions and MC sample variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.hist(predictions)\n",
    "plt.title(\"Distribution of p values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_var = np.std(pred_probs_mc, axis=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(mc_var)\n",
    "plt.title(\"Distribution of MC variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs for false predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_idx = np.where(targets != pred_labels)[0]\n",
    "err_predictions = predictions[err_idx]\n",
    "err_mc_samples = pred_probs_mc[err_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(err_idx)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=n, figsize=(n*2.5, 2.5))\n",
    "\n",
    "for (i, _ax) in enumerate(ax):\n",
    "    x = testset[err_idx[i]][0][0]\n",
    "    _ax.imshow(x)\n",
    "    _ax.set_title(\"s = {:.3f} +/- {:.3f}\".format(err_predictions[i], err_mc_samples[i,:].std() / np.sqrt(err_mc_samples[i,:].shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the predicted scores and their confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_idx = np.where(targets != pred_labels)[0]\n",
    "\n",
    "mc = pred_probs_mc.shape[1]\n",
    "unc = np.std(pred_probs_mc, axis=1) / np.sqrt(mc)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.scatter(predictions, unc, s=10)\n",
    "plt.scatter(predictions[err_idx], unc[err_idx], c='r', s=50)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 0.1)\n",
    "\n",
    "plt.xlabel(\"Predicted value of p\")\n",
    "plt.ylabel(\"Uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with blurred images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bnn_model(model, blur_levels = [1.0, 2.0, 3.0, 4.0, 5.0]):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1 + len(blur_levels), figsize=(6 * (1 + len(blur_levels)), 4))\n",
    "    # -----------------------------------------------------------\n",
    "    _ax = ax[0]\n",
    "    testset = datasets.BinaryMNISTC(53, 'identity', 'test', transform=normalize_x())\n",
    "    testloader = DataLoader(dataset=testset, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    pred_labels, targets, predictions, pred_probs_mc = evaluate_model(model, testloader)\n",
    "    acc = (targets == pred_labels).mean()\n",
    "    print(\"Test accuracy: \", acc)\n",
    "\n",
    "    pred_probs_mc = pred_probs_mc[:, :, 1]\n",
    "    predictions = predictions[:, 1]\n",
    "        \n",
    "    err_idx = np.where(targets != pred_labels)[0]\n",
    "    mc = pred_probs_mc.shape[1]\n",
    "    unc = np.std(pred_probs_mc, axis=1) / np.sqrt(mc)\n",
    "    _ax.scatter(predictions, unc, s=10)\n",
    "    _ax.scatter(predictions[err_idx], unc[err_idx], c='r', s=50)\n",
    "    _ax.set_xlim(-0.05, 1.05)\n",
    "    _ax.set_ylim(0.0, 0.10)\n",
    "    _ax.set_xlabel(\"Predicted value of p\")\n",
    "    _ax.set_ylabel(\"Uncertainty\")\n",
    "    _ax.set_title(\"Clean data (Acc = {:.4f})\".format(acc))\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    for i, blur_std in enumerate(blur_levels):\n",
    "        x_transform = normalize_and_blur(std=blur_std)\n",
    "        testset = datasets.BinaryMNISTC(53, 'identity', 'test', transform=x_transform)\n",
    "        testloader = DataLoader(dataset=testset, batch_size=1024, shuffle=False)\n",
    "\n",
    "        pred_labels, targets, predictions, pred_probs_mc = evaluate_model(model, testloader)\n",
    "        acc = (targets == pred_labels).mean()\n",
    "        print(\"Test accuracy: \", acc)\n",
    "\n",
    "        pred_probs_mc = pred_probs_mc[:, :, 1]\n",
    "        predictions = predictions[:, 1]\n",
    "\n",
    "        err_idx = np.where(targets != pred_labels)[0]\n",
    "        mc = pred_probs_mc.shape[1]\n",
    "        unc = np.std(pred_probs_mc, axis=1) / np.sqrt(mc)\n",
    "        \n",
    "        _ax = ax[i+1]\n",
    "        _ax.scatter(predictions, unc, s=10)\n",
    "        _ax.scatter(predictions[err_idx], unc[err_idx], c='r', s=50)\n",
    "        _ax.set_xlim(-0.05, 1.05)\n",
    "        _ax.set_ylim(0.0, 0.10)\n",
    "        _ax.set_xlabel(\"Predicted value of p\")\n",
    "        _ax.set_ylabel(\"Uncertainty\")\n",
    "        _ax.set_title(\"Blur {} (Acc = {:.4f})\".format(blur_std, acc))\n",
    "        \n",
    "    #--\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = analyze_bnn_model(model, blur_levels=[0.50, 1.0, 2.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transform = normalize_and_blur(std=5.0)\n",
    "testset = datasets.BinaryMNISTC(53, 'identity', 'test', transform=x_transform)\n",
    "testloader = DataLoader(dataset=testset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x, y = next(iter(testloader))\n",
    "x = x[:n]\n",
    "y = y[:n]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=n, figsize=(n*2.5, 2.5))\n",
    "\n",
    "for (i, _ax) in enumerate(ax):\n",
    "    _x = x[i, 0]\n",
    "    _ax.imshow(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels, targets, predictions, pred_probs_mc = evaluate_model(model, testloader)\n",
    "print(\"Test accuracy: \", (targets == pred_labels).mean())\n",
    "\n",
    "pred_probs_mc = pred_probs_mc[:, :, 1]\n",
    "predictions = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_idx = np.where(targets != pred_labels)[0]\n",
    "\n",
    "mc = pred_probs_mc.shape[1]\n",
    "unc = np.std(pred_probs_mc, axis=1) / np.sqrt(mc)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.scatter(predictions, unc, s=10)\n",
    "plt.scatter(predictions[err_idx], unc[err_idx], c='r', s=50)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 0.1)\n",
    "\n",
    "plt.xlabel(\"Predicted value of p\")\n",
    "plt.ylabel(\"Uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check SL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = \"./../zoo/alpha-sl-scaling/BinaryMNISTC-53-identity/LeNet/sl-lam1e+00-alpha1e+02-nw-5-20220401055444\"\n",
    "# model_dir = \"./../zoo/alpha-sl-scaling/BinaryMNISTC-53-identity/LeNet/sl-lam1e-05-alpha1e+01-nw-1-20220401045543\"\n",
    "model_dir = \"./../zoo/alpha-sl-scaling/BinaryMNISTC-53-identity/LeNet/sl-lam1e-02-alpha1e+00-nw-5-20220401033018\"\n",
    "\n",
    "# Default paths\n",
    "config_json = os.path.join(model_dir, \"config.json\")\n",
    "ckpt_file = os.path.join(model_dir, \"last.ckpt\")\n",
    "\n",
    "config = json.load(open(config_json, 'r'))\n",
    "\n",
    "MethodClass = getattr(methods, config['method'])\n",
    "DatasetClass = getattr(datasets, config['dataset'])\n",
    "ModelClass = getattr(models, config['model'])\n",
    "TransformClass = normalize_x# getattr(transforms, config['transform'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MethodClass.load_from_checkpoint(\n",
    "            os.path.join(model_dir, \"last.ckpt\"),\n",
    "            model=ModelClass(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = datasets.BinaryMNISTC(53, 'identity', 'test', transform=TransformClass())\n",
    "testloader = DataLoader(dataset=testset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x, y = next(iter(testloader))\n",
    "x = x[:n]\n",
    "y = y[:n]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=n, figsize=(n*2.5, 2.5))\n",
    "\n",
    "for (i, _ax) in enumerate(ax):\n",
    "    _x = x[i, 0]\n",
    "    _ax.imshow(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels, targets, predictions, pred_probs_mc = evaluate_model(model, testloader)\n",
    "print(\"Test accuracy: \", (targets == pred_labels).mean())\n",
    "\n",
    "pred_probs_mc = pred_probs_mc[:, :, 1]\n",
    "predictions = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_idx = np.where(targets != pred_labels)[0]\n",
    "\n",
    "mc = pred_probs_mc.shape[1]\n",
    "unc = np.std(pred_probs_mc, axis=1) / np.sqrt(mc)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.scatter(predictions, unc, s=10)\n",
    "plt.scatter(predictions[err_idx], unc[err_idx], c='r', s=50)\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(0.0, 0.10)\n",
    "\n",
    "plt.xlabel(\"Predicted value of p\")\n",
    "plt.ylabel(\"Uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transform = normalize_and_blur(std=5.0)\n",
    "testset = datasets.BinaryMNISTC(53, 'identity', 'test', transform=x_transform)\n",
    "testloader = DataLoader(dataset=testset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "x, y = next(iter(testloader))\n",
    "x = x[:n]\n",
    "y = y[:n]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=n, figsize=(n*2.5, 2.5))\n",
    "\n",
    "for (i, _ax) in enumerate(ax):\n",
    "    _x = x[i, 0]\n",
    "    _ax.imshow(_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels, targets, predictions, pred_probs_mc = evaluate_model(model, testloader)\n",
    "print(\"Test accuracy: \", (targets == pred_labels).mean())\n",
    "\n",
    "pred_probs_mc = pred_probs_mc[:, :, 1]\n",
    "predictions = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_idx = np.where(targets != pred_labels)[0]\n",
    "\n",
    "mc = pred_probs_mc.shape[1]\n",
    "unc = np.std(pred_probs_mc, axis=1) / np.sqrt(mc)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.scatter(predictions, unc, s=10)\n",
    "plt.scatter(predictions[err_idx], unc[err_idx], c='r', s=50)\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 0.15)\n",
    "\n",
    "plt.xlabel(\"Predicted value of p\")\n",
    "plt.ylabel(\"Uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model_dir, blur_levels = [1.0, 2.0, 3.0, 4.0, 5.0]):\n",
    "    \n",
    "    # Default paths\n",
    "    config_json = os.path.join(model_dir, \"config.json\")\n",
    "    ckpt_file = os.path.join(model_dir, \"last.ckpt\")\n",
    "    config = json.load(open(config_json, 'r'))\n",
    "\n",
    "    MethodClass = getattr(methods, config['method'])\n",
    "    DatasetClass = getattr(datasets, config['dataset'])\n",
    "    ModelClass = getattr(models, config['model'])\n",
    "    TransformClass = normalize_x# getattr(transforms, config['transform'])\n",
    "    \n",
    "    model = MethodClass.load_from_checkpoint(\n",
    "            os.path.join(model_dir, \"last.ckpt\"),\n",
    "            model=ModelClass(2))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1 + len(blur_levels), figsize=(6 * (1 + len(blur_levels)), 4))\n",
    "    # -----------------------------------------------------------\n",
    "    _ax = ax[0]\n",
    "    testset = datasets.BinaryMNISTC(53, 'identity', 'test', transform=TransformClass())\n",
    "    testloader = DataLoader(dataset=testset, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    pred_labels, targets, predictions, pred_probs_mc = evaluate_model(model, testloader)\n",
    "    acc = (targets == pred_labels).mean()\n",
    "    print(\"Test accuracy: \", acc)\n",
    "\n",
    "    pred_probs_mc = pred_probs_mc[:, :, 1]\n",
    "    predictions = predictions[:, 1]\n",
    "        \n",
    "    err_idx = np.where(targets != pred_labels)[0]\n",
    "    mc = pred_probs_mc.shape[1]\n",
    "    unc = np.std(pred_probs_mc, axis=1) / np.sqrt(mc)\n",
    "    _ax.scatter(predictions, unc, s=10)\n",
    "    _ax.scatter(predictions[err_idx], unc[err_idx], c='r', s=50)\n",
    "    _ax.set_xlim(-0.05, 1.05)\n",
    "    _ax.set_ylim(0.0, 0.10)\n",
    "    _ax.set_xlabel(\"Predicted value of p\")\n",
    "    _ax.set_ylabel(\"Uncertainty\")\n",
    "    _ax.set_title(\"Clean data (Acc = {:.4f})\".format(acc))\n",
    "    \n",
    "    # ------------------------------------------------------------\n",
    "    for i, blur_std in enumerate(blur_levels):\n",
    "        x_transform = normalize_and_blur(std=blur_std)\n",
    "        testset = datasets.BinaryMNISTC(53, 'identity', 'test', transform=x_transform)\n",
    "        testloader = DataLoader(dataset=testset, batch_size=1024, shuffle=False)\n",
    "\n",
    "        pred_labels, targets, predictions, pred_probs_mc = evaluate_model(model, testloader)\n",
    "        acc = (targets == pred_labels).mean()\n",
    "        print(\"Test accuracy: \", acc)\n",
    "\n",
    "        pred_probs_mc = pred_probs_mc[:, :, 1]\n",
    "        predictions = predictions[:, 1]\n",
    "\n",
    "        err_idx = np.where(targets != pred_labels)[0]\n",
    "        mc = pred_probs_mc.shape[1]\n",
    "        unc = np.std(pred_probs_mc, axis=1) / np.sqrt(mc)\n",
    "        \n",
    "        _ax = ax[i+1]\n",
    "        _ax.scatter(predictions, unc, s=10)\n",
    "        _ax.scatter(predictions[err_idx], unc[err_idx], c='r', s=50)\n",
    "        _ax.set_xlim(-0.05, 1.05)\n",
    "        _ax.set_ylim(0.0, 0.10)\n",
    "        _ax.set_xlabel(\"Predicted value of p\")\n",
    "        _ax.set_ylabel(\"Uncertainty\")\n",
    "        _ax.set_title(\"Blur {} (Acc = {:.4f})\".format(blur_std, acc))\n",
    "        \n",
    "    #--\n",
    "    lam_sl = config['method_params']['lam_sl']\n",
    "    alpha = config['method_params']['alpha']\n",
    "    plt.suptitle(\"$\\\\lambda_{} = {:1.0e} \\\\qquad    \\\\alpha = {:1.0e}$\".format(\"{SL}\", lam_sl, alpha))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./../zoo/alpha-sl-scaling/BinaryMNISTC-53-identity/LeNet/sl-lam1e-05-alpha1e+00-nw-3-20220401035110\"\n",
    "fig = analyze_model(model_dir, blur_levels=[0.50, 1.0, 2.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./../zoo/alpha-sl-scaling/BinaryMNISTC-53-identity/LeNet/sl-lam1e-04-alpha1e+00-nw-1-20220401034326\"\n",
    "fig = analyze_model(model_dir, blur_levels=[0.50, 1.0, 2.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./../zoo/alpha-sl-scaling/BinaryMNISTC-53-identity/LeNet/sl-lam1e-03-alpha1e+00-nw-5-20220401032602\"\n",
    "fig = analyze_model(model_dir, blur_levels=[0.50, 1.0, 2.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./../zoo/alpha-sl-scaling/BinaryMNISTC-53-identity/LeNet/sl-lam1e-02-alpha1e+00-nw-1-20220401035306\"\n",
    "fig = analyze_model(model_dir, blur_levels=[0.50, 1.0, 2.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./../zoo/alpha-sl-scaling/BinaryMNISTC-53-identity/LeNet/sl-lam1e-01-alpha1e+00-nw-1-20220401035756\"\n",
    "fig = analyze_model(model_dir, blur_levels=[0.50, 1.0, 2.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./../zoo/alpha-sl-scaling/BinaryMNISTC-53-identity/LeNet/sl-lam1e+00-alpha1e+00-nw-1-20220401040245\"\n",
    "fig = analyze_model(model_dir, blur_levels=[0.50, 1.0, 2.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bayes')",
   "language": "python",
   "name": "python397jvsc74a57bd013ede2aa3846a80420e8be59016e3b86d734ba47be24b4292330ac6ad96136da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
